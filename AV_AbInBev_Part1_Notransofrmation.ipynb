{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load libs\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import math, keras, datetime, pandas as pd, numpy as np, keras.backend as K\n",
    "import matplotlib.pyplot as plt, operator, random, pickle\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "from isoweek import Week\n",
    "from pandas_summary import DataFrameSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"F:/AV/AbInBev/\"\n",
    "train_folder = 'train_OwBvO8W'\n",
    "test_folder = 'test_8uviCCm'\n",
    "submit_folder = 'sample_submission_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.walk(path+train_folder):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functoin to get csvs within a folder\n",
    "\n",
    "def get_csv(path,folder):\n",
    "    for path,directory,files in os.walk(path+folder):\n",
    "        return(path,directory,[path+\"/\"+file for file in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train details\n",
    "tr_path,_,tr_files=get_csv(path,train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tables_csv\n",
    "train_tables = [pd.read_csv(file,low_memory=False) for file in tr_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### take a look at head of table\n",
    "for t in train_tables: display(t.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lets check the summary for train tables\n",
    "\n",
    "for t in train_tables: display(DataFrameSummary(t).summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### no missing values in any field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning/Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### separate out into each individual table\n",
    "demographics,event_cal,hist_volm,ind_soda_sales,ind_volm,promo,weather = train_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will create multiple columns indicating which weeks had which kind of holiday/event and a combination of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lets add more info to event_calendar dataframe based on state holidays, sports, music festivals, beer related stuff\n",
    "state_hols = ['Easter Day','Good Friday','New Year','Christmas','Labor Day','Independence Day','Revolution Day Memorial']\n",
    "sports = ['Regional Games ','FIFA U-17 World Cup','Football Gold Cup']\n",
    "\n",
    "event_cal['National_Hol'] = np.where(np.sum(event_cal[state_hols],axis=1)>0,1,0) \n",
    "event_cal['Sports_Week'] = np.where(np.sum(event_cal[sports],axis=1)>0,1,0) \n",
    "\n",
    "event_cal['National_Hol_and_Sports_Week'] = np.where(np.sum(event_cal[['National_Hol','Sports_Week']],axis=1)>1,1,0) \n",
    "event_cal['Beer_Capital_and_Sports'] = np.where(np.sum(event_cal[['Beer Capital','Sports_Week']],axis=1)>1,1,0) \n",
    "event_cal['Beer_Capital_and_Music Fest'] = np.where(np.sum(event_cal[['Beer Capital','Music Fest']],axis=1)>1,1,0) \n",
    "event_cal['Sports_Week_and_Music Fest'] = np.where(np.sum(event_cal[['Sports_Week','Music Fest']],axis=1)>1,1,0) \n",
    "event_cal['National_Hol_and_Music Fest'] = np.where(np.sum(event_cal[['National_Hol','Music Fest']],axis=1)>1,1,0) \n",
    "event_cal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function to retrieve last event for all yearmonths\n",
    "\n",
    "def prior_event_track(df,event):\n",
    "    col = []\n",
    "    last_YearMonth = 201301 #000000\n",
    "    last_event_val = df.iloc[0,:][event]     \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if True:\n",
    "            curr_val_event = df.iloc[i,:][event]        \n",
    "            curr_YearMonth = df.iloc[i,:]['YearMonth']\n",
    "#             print(curr_val_event)\n",
    "#             print(curr_YearMonth)   \n",
    "            \n",
    "        if curr_val_event == 0 :\n",
    "            mth_begin = str(curr_YearMonth)[4:]\n",
    "            mth_end   = str(last_YearMonth)[4:]\n",
    "            yr_begin = str(curr_YearMonth)[:4]\n",
    "            yr_end   = str(last_YearMonth)[:4]\n",
    "            \n",
    "            mth_diff = int(mth_begin) - int(mth_end)\n",
    "            yr_diff = int(yr_begin) - int(yr_end)\n",
    "            \n",
    "            if yr_diff>0:\n",
    "                mth_diff += yr_diff*12\n",
    "            if last_event_val == 0:\n",
    "                col.append(99)\n",
    "            if last_event_val != 0:\n",
    "                col.append(mth_diff)\n",
    "            \n",
    "        if curr_val_event > 0 :\n",
    "            last_YearMonth = curr_YearMonth\n",
    "            last_event_val = curr_val_event\n",
    "            col.append(0)\n",
    "    \n",
    "    df['time_since_'+event] = col        \n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add all prior event info to useful general columns\n",
    "\n",
    "prior_event_track(event_cal,'National_Hol')\n",
    "prior_event_track(event_cal,'Sports_Week')\n",
    "prior_event_track(event_cal,'National_Hol_and_Sports_Week')\n",
    "prior_event_track(event_cal,'Beer_Capital_and_Sports')\n",
    "prior_event_track(event_cal,'Beer_Capital_and_Music Fest')\n",
    "prior_event_track(event_cal,'Music Fest')\n",
    "event_cal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add per capita income to demographics table\n",
    "\n",
    "demographics['Per_Capita'] = demographics['Avg_Yearly_Household_Income_2017']/demographics['Avg_Population_2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets also see how each row/Agency's demographcis relate to overall max and min for pop and income\n",
    "def update_demographics_max_min(dataframe,columns):\n",
    "    for column in columns:\n",
    "        feat_max =  dataframe[column].max()\n",
    "        feat_min =  dataframe[column].min()\n",
    "        feat_avg =  dataframe[column].mean()\n",
    "        dataframe[column+\"_max_diff\"] = feat_max - dataframe[column]\n",
    "        dataframe[column+\"_min_diff\"] = feat_min - dataframe[column]\n",
    "        dataframe[column+\"_avg_diff\"] = feat_avg - dataframe[column]\n",
    "        del feat_max,feat_min,feat_avg\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = update_demographics_max_min(demographics,['Avg_Population_2017','Avg_Yearly_Household_Income_2017','Per_Capita'])\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lets try facebook's prophet forecasting lib\n",
    "import datetime\n",
    "from fbprophet import Prophet\n",
    "\n",
    "### we need 2 columns for each agency: namely DS and Y (DS is date and Y is numeric)\n",
    "weather['YearMonth'] = weather['YearMonth'].astype(str)+('01')\n",
    "weather['ds'] = pd.to_datetime(weather['YearMonth'].astype(str), format='%Y%m%d')\n",
    "weather['y'] = weather['Avg_Max_Temp']\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### predict last data point for max temp for 201801 (takes a min or two)\n",
    "pred_temp = []\n",
    "for agency in np.unique(weather['Agency']):\n",
    "    # agency = 'Agency_60'\n",
    "    agency = agency\n",
    "    Avg_Temp = weather.loc[weather['Agency']==agency]\n",
    "    m= Prophet().fit(Avg_Temp)\n",
    "    future = m.make_future_dataframe(periods=1,freq = 'M')\n",
    "    fcst = m.predict(future)\n",
    "    m.plot(fcst)\n",
    "    pred = fcst.iloc[-1,-1]\n",
    "    pred_temp.append({agency:pred})\n",
    "    del m, Avg_Temp,fcst,future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build am agency location dataframe\n",
    "pred_temp_df = pd.DataFrame(np.zeros((len(pred_temp), 2)))\n",
    "pred_temp_df.columns = ['Agency',\"Avg_Max_Temp\"]\n",
    "idx = 0\n",
    "for i in pred_temp:\n",
    "    #print(i)\n",
    "    for k,v in i.items():\n",
    "        #print(k,v)\n",
    "        pred_temp_df.iloc[idx,0] = k\n",
    "        pred_temp_df.iloc[idx,1] = v\n",
    "    idx += 1\n",
    "\n",
    "pred_temp_df['YearMonth'] = '20180101'\n",
    "pred_temp_df = pred_temp_df[['YearMonth','Agency','Avg_Max_Temp']]\n",
    "pred_temp_df.head()\n",
    "\n",
    "### Seems like some agencies are in the same location, lets add that info \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merge the 201801 prediction with actual file\n",
    "\n",
    "weather.drop(['ds','y'],inplace=True,axis=1)\n",
    "weather = pd.concat([weather,pred_temp_df])\n",
    "weather['YearMonth'] = weather['YearMonth'].astype(str).str[:-2].astype(np.int64)\n",
    "weather.head()\n",
    "### this file has original data and prediction for final month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from https://stackoverflow.com/questions/20672238/find-dictionary-keys-with-duplicate-values\n",
    "\n",
    "### output: list with unqiue next month temp pred value and the respective agency\n",
    "\n",
    "flipped = {}\n",
    "for i in pred_temp:\n",
    "    for key, value in i.items():\n",
    "        if value not in flipped:\n",
    "            flipped[value] = [key]\n",
    "        else:\n",
    "            flipped[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_location = []\n",
    "i = 0\n",
    "for k,v in flipped.items():\n",
    "    for item in v:\n",
    "        agency_location.append({item:\"location\"+str(i)})\n",
    "        #print(v)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build an agency location dataframe --- this is only a hypothesis\n",
    "\n",
    "Agency_Location_DF = pd.DataFrame(np.zeros((len(agency_location), 2)))\n",
    "Agency_Location_DF.columns = ['Agency',\"Location\"]\n",
    "idx = 0\n",
    "for i in agency_location:\n",
    "    print(i)\n",
    "    for k,v in i.items():\n",
    "        print(k,v)\n",
    "        Agency_Location_DF.iloc[idx,0] = k\n",
    "        Agency_Location_DF.iloc[idx,1] = v\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agency_Location_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets merge the weather and location dataframes\n",
    "Weather_Location_merge = weather.merge(Agency_Location_DF,how='left',on=\"Agency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will not look into other files such a soda vol, industry volume but will work on the below:\n",
    "\n",
    "- Agency_Location_DF\n",
    "- weather\n",
    "- demographics\n",
    "- event_cal\n",
    "- hist_volm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_volm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline --- Not using other files only historical volumnes and facebook package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_volm_upd = hist_volm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_volm_upd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we need 2 columns for each agency: namely DS and Y (DS is date and Y is numeric)\n",
    "hist_volm_upd['YearMonth'] = hist_volm_upd['YearMonth'].astype(str)+('01')\n",
    "hist_volm_upd['ds'] = pd.to_datetime(hist_volm_upd['YearMonth'].astype(str), format='%Y%m%d')\n",
    "hist_volm_upd['y'] = hist_volm_upd['Volume']\n",
    "hist_volm_upd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_volm_upd['Agency_SKU'] = hist_volm_upd['Agency']+\"-\"+hist_volm_upd['SKU']\n",
    "hist_volm_upd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### predict last data point for each agency - SKU combination\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pred_volm = []\n",
    "\n",
    "for agency_sku in tqdm(np.unique(hist_volm_upd['Agency_SKU'])):\n",
    "    agency,sku = agency_sku.split(\"-\")\n",
    "    pred = 0\n",
    "    rt = 0\n",
    "    sku = sku\n",
    "    agency = agency\n",
    "\n",
    "    Volm_df = hist_volm_upd.loc[(hist_volm_upd['Agency']==agency) & (hist_volm_upd['SKU']==sku)]\n",
    "\n",
    "    if len(Volm_df) == 0:\n",
    "        pred = 0\n",
    "        pred_volm.append({agency+\"-\"+sku:int(pred)})\n",
    "        rt +=1\n",
    "        #print(rt)\n",
    "\n",
    "    elif ((len(Volm_df.loc[Volm_df['YearMonth']=='20171201'])>0) & ((Volm_df.loc[Volm_df['YearMonth']=='20171201'])['Volume'].values==0)):\n",
    "        pred = 0\n",
    "        pred_volm.append({agency+\"-\"+sku:int(pred)})\n",
    "        rt +=1\n",
    "        #print(rt)\n",
    "\n",
    "    else:\n",
    "        m = Prophet().fit(Volm_df)\n",
    "        future = m.make_future_dataframe(periods=1,freq = 'M')\n",
    "        fcst = m.predict(future)\n",
    "        m.plot(fcst)\n",
    "        print(fcst)\n",
    "        pred = fcst.iloc[-1,-1]\n",
    "        if pred <0:\n",
    "            pred = Volm_df.loc[Volm_df['YearMonth']=='20171201']['Volume'].values\n",
    "        del m,fcst,future\n",
    "        pred_volm.append({agency_sku:int(pred)})\n",
    "        rt +=1\n",
    "        print(agency_sku,pred)\n",
    "        \n",
    "    print('final rt:',rt)\n",
    "    del Volm_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### predict last data point for each agency - SKU combination\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pred_volm = []\n",
    "\n",
    "# for agency in tqdm(np.unique(hist_volm_upd['Agency'])):\n",
    "    \n",
    "#     for sku in np.unique(hist_volm_upd['SKU']):\n",
    "        \n",
    "#         pred = 0\n",
    "#         rt = 0\n",
    "#         sku = sku\n",
    "#         agency = agency\n",
    "        \n",
    "#         Volm_df = hist_volm_upd.loc[(hist_volm_upd['Agency']==agency) & (hist_volm_upd['SKU']==sku)]\n",
    "        \n",
    "#         if len(Volm_df) == 0:\n",
    "#             pred = 0\n",
    "#             pred_volm.append({agency+\"_\"+sku:pred})\n",
    "#             rt +=1\n",
    "#             #print(rt)\n",
    "            \n",
    "#         elif ((len(Volm_df.loc[Volm_df['YearMonth']=='20171201'])>0) & ((Volm_df.loc[Volm_df['YearMonth']=='20171201'])['Volume'].values==0)):\n",
    "#             pred = 0\n",
    "#             pred_volm.append({agency+\"_\"+sku:pred})\n",
    "#             rt +=1\n",
    "#             #print(rt)\n",
    "            \n",
    "#         else:\n",
    "#             m = Prophet().fit(Volm_df)\n",
    "#             future = m.make_future_dataframe(periods=1,freq = 'M')\n",
    "#             fcst = m.predict(future)\n",
    "#             m.plot(fcst)\n",
    "#             pred = fcst.iloc[-1,-1]\n",
    "#             if pred <0:\n",
    "#                 pred = Volm_df.loc[Volm_df['YearMonth']=='20171201']['Volume']\n",
    "#             del m,fcst,future\n",
    "#             pred_volm.append({agency+\"_\"+sku:pred})\n",
    "#             rt +=1\n",
    "#             #print(rt)\n",
    "#         print('final rt:',rt)\n",
    "#         del Volm_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_volm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pred_volm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build am agency location dataframe\n",
    "pred_temp_df = pd.DataFrame(np.zeros((len(pred_volm), 2)))\n",
    "pred_temp_df.columns = ['Agency_SKU',\"Volume\"]\n",
    "idx = 0\n",
    "for i in pred_volm:\n",
    "    for k,v in i.items():\n",
    "        print(k,v)\n",
    "        pred_temp_df.iloc[idx,0] = k\n",
    "        pred_temp_df.iloc[idx,1] = v\n",
    "        idx += 1\n",
    "\n",
    "#pred_temp_df['YearMonth'] = '20180101'\n",
    "#pred_temp_df = pred_temp_df[['YearMonth','Agency','Avg_Max_Temp']]\n",
    "pred_temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for agency in tqdm(np.unique(hist_volm_upd['Agency'])):\n",
    "    for sku in np.unique(hist_volm_upd['SKU']):\n",
    "        if agency+\"-\"+sku not in np.unique(pred_temp_df['Agency_SKU']):\n",
    "            tp = agency+\"-\"+sku\n",
    "            temp.append({tp:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build am agency location dataframe\n",
    "pred_temp_df1 = pd.DataFrame(np.zeros((len(temp), 2)))\n",
    "pred_temp_df1.columns = ['Agency_SKU',\"Volume\"]\n",
    "idx = 0\n",
    "for i in temp:\n",
    "    for k,v in i.items():\n",
    "        print(k,v)\n",
    "        pred_temp_df1.iloc[idx,0] = k\n",
    "        pred_temp_df1.iloc[idx,1] = v\n",
    "        idx += 1\n",
    "\n",
    "#pred_temp_df['YearMonth'] = '20180101'\n",
    "#pred_temp_df = pred_temp_df[['YearMonth','Agency','Avg_Max_Temp']]\n",
    "pred_temp_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([pred_temp_df,pred_temp_df1],axis=0)\n",
    "submit=pd.DataFrame(submit)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit[['Agency','SKU']] = submit['Agency_SKU'].str.split('-',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.drop('Agency_SKU', inplace=True)\n",
    "submit = submit[['Agency','SKU','Volume']]\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('F:/AV/AbInBev/submit/'+'volume_forecast_v2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
